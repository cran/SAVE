% -*- mode: noweb; noweb-default-code-mode: R-mode; -*-
%\VignetteIndexEntry{The SAVE Package}
%\VignetteDepends{coda, DiceKriging, methods}
%\VignetteKeywords{R package, computer models, calibration, emulation,
%Bayesian analysis, Gaussian processes}
%\VignettePackage{SAVE}

\documentclass[nojss]{jss}
\usepackage{amsmath}
\usepackage[utf8x]{inputenc}

\newcommand{\ourpack}{\pkg{SAVE}}
\newcommand{\n}[1]{\mbox{\boldmath{$#1$}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%almost as usual
\author{Jesus Palomo\\URJC\And Rui Paulo\\ISEG and CEMAPRE\And Gonzalo
Garc\'\i a-Donato\\UCLM} 
\title{\pkg{SAVE}: an \proglang{R} package for the Statistical Analysis of Computer Models} 
%\VignetteIndexEntry{The SAVE package}


%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Jes\'us Palomo, Rui Paulo, Gonzalo Garc\'\i a-Donato} %%
% comma-separated
\Plaintitle{SAVE: an R package for the Statistical Analysis of
  Computer Models} %% without formatting 
\Shorttitle{\pkg{SAVE}: Analysis of Computer Models} %% a short
                                %% title (if necessary) 

%% an abstract and keywords
\Abstract{
  This paper introduces the \proglang{R} package \pkg{SAVE} which
  implements statistical methodology for the analysis of computer
  models. Namely, the package includes routines that perform
  emulation, calibration and 
  validation of this type of models. The methodology is Bayesian and
  is essentially that of \citet{Baya:2007}. The package is available
  through the Comprehensive R Archive Network, CRAN. We illustrate its
  use with a real data example.

 }

\Keywords{\proglang{R} package, computer models, calibration, emulation,
  Bayesian analysis, Gaussian processes} 
\Plainkeywords{R package, computer models, calibration, emulation,
  Bayesian analysis, Gaussian processes} %% without formatting   
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
Jesus Palomo\\
Department of Business Administration (Finance)\\
Rey Juan Carlos University\\
28032 Madrid, Spain\\
Email: \email{jesus.palomo@urjc.es}

%\Address{
Rui Paulo\\
Department of Mathematics and CEMAPRE\\
ISEG, Technical University of Lisbon\\
1200 Lisboa, Portugal\\
Email: \email{rui@iseg.utl.pt}

%\Address{
Gonzalo Garc\'\i a-Donato\\
Department of Economics and Finance\\
Universidad de Castilla-La Mancha\\
02071, Albacete, Spain\\
Email: \email{gonzalo.garciadonato@uclm.es}}

%  Achim Zeileis\\
 % Department of Statistics and Mathematics\\
  %Faculty of Economics and Statistics\\
  %Universit\"at Innsbruck\\
  %6020 Innsbruck, Austria\\
  %E-mail: \email{Achim.Zeileis@uibk.ac.at}\\
  %URL: \url{http://eeecon.uibk.ac.at/~zeileis/}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\SweaveOpts{}

\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.
%\section[About Java]{About \proglang{Java}}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.

\section{The analysis of complex computer models}
Complex computer models are implementations of sophisticated
mathematical models that aim at reproducing a particular real
process. The \proglang{R} package \ourpack\ (\textbf{S}tatistical
\textbf{A}nalysis and 
\textbf{V}alidation \textbf{E}ngine) implements
statistical methodology developed for the analysis of this type of
models, which is based on \citet{Crai:1996},
\citet{Kenn:OHag:2001}, \citet{Kenn:OHag:Higg:2002},
\citet{Higd:2004}, and most directly on \citet{Baya:2007}. The package
is available through the Comprehensive R Archive Network, CRAN.

The following aspects of the statistical analysis of a computer model
are addressed in \ourpack:
\begin{itemize}
\item {\em Emulation}. A crucial characteristic of these models is
  that they are often computationally very demanding and a single run
  may take several minutes to complete. It is then
  important to produce fast approximations to the output of these
  models, and these approximations are referred to as emulators.
\item {\em Calibration}. Computer models usually depend on a vector of
  unknown inputs that needs to be specified before the model can be
  run. Calibration refers to the process of determining estimates of
  these calibration parameters based on field observations of the real
  process.
\item {\em Validation}. Ultimately, we want to assess the degree to
  which the computer model is an effective surrogate for the real
  process. We do so by producing predictions of reality and associated
  tolerance bounds, which measure the degree of the
  accuracy of said predictions.
\end{itemize}

Related \proglang{R} packages include \pkg{BACCO}, \citet{bacco}, and
the suite of \pkg{Dice} packages: \pkg{DiceKrigging} and 
\pkg{DiceOptim} \citep{dice12}, \pkg{DiceEval} \citep{DE} and
\pkg{DiceDesign} \citep{DD}. \pkg{DiceKrigging}
computes estimates of Gaussian process parameters, and our package
takes advantage of its functionalities, while \pkg{DiceOptim} is
dedicated to optimimization of complex computer models based on
krigging models. 
\pkg{DiceDesign} 
facilitates the construction of space-filling designs
for computer experiments. \pkg{DiceEval} tackles the problem of 
validation but follows an approach that is quite different from ours;
in particular, it is not Bayesian. 

\pkg{BACCO} is a package that is similar to ours in its
goals. However, it
implements the methodology in \citet{Kenn:OHag:2001} 
which, although Bayesian, is distinct from that of \citet{Baya:2007},
the one we implement [\pkg{BACCO} also implements the methods in
\citet{Kenn:O'Ha:2000}, which is a topic we do not cover]. The most
important distinction between \pkg{BACCO} and \pkg{SAVE} is that 
we explore the posterior distribution of the parameters of the
statistical model
using simulation-based techniques, namely Markov chain Monte Carlo,
whereas \pkg{BACCO} relies either 
on analytical or on numerical integration. In that
sense, with \pkg{SAVE} one can for instance explore the posterior
distribution of calibration parameters, which often have a physical
meaning, and take advantage of all the benefits that come with
simulation-based inference. This will be illustrated in 
Section~\ref{spotweld}.

We should note that the \pkg{SAVE} package relies
on \proglang{C} code to perform computer intensive
calculations. Additionally, in order to maintain numerical
stability as much as possible, we make from our own \proglang{C} code
extensive calls to numerical 
routines written in \proglang{Fortran}, notably those available from
\proglang{BLAS} and \proglang{LAPACK}.

% The package \ourpack\ is available from the Comprehensive R Archive
% Network at {\tt http://CRAN.R-project.org/package=SAVE} and can be
% installed using  
% \begin{CodeInput}
% R> install.packages(SAVE)
% \end{CodeInput}
% Once it is installed, the user can have access to the functionalities
% in \ourpack\ through the command: 
% \begin{CodeInput}
% R> library(SAVE)
% \end{CodeInput}

The rest of this paper is organized as follows. In the next section we
describe the statistical methodology establishing
links with the package. In Section~\ref{overview} we describe the
structure of the package, and in
Section~\ref{spotweld} we illustrate its use in the context of a real
example. Technical details are, whenever possible, relegated to the
appendices. 

\section{Introducing the statistical framework}\label{frame}
Denote the output of the computer model by $y^M (\n x, \n u)$, where
$\n x$ is a 
vector of controllable inputs and  
$\n u$ is a vector of unknown calibration and/or tuning parameters in
the model. We have
access to the data obtained by evaluating the computer model at a
design set 
consisting of $N$ points $D^M=\{(\n x_1,\n u_1),\ldots,(\n x_{N},\n
u_{N})\}$. % Model output and associated design are stored in a {\tt
%   data.frame} which is called {\tt model.data} in this paper. This
% object has $N$ rows;
% the columns contain the response output and the input
% variables. This is illustrated in Table~\ref{inv.dataframe} where, for
% pedagogical reasons, a simple synthetic example is used. 
We denote by
$\n y^M$ the vector of model evaluations.

A preliminary but central question in the analysis is the construction
of an emulator of the computer model, that is, a method to produce
estimates of the output at untested configurations along with an
associated measure of uncertainty. This is stage I of the analysis of
a computer model. For this task, we follow the
popular strategy (cf. \citet{Sack:1989}, \citet{Kenn:O'Ha:2000} and
\citet{Baya:2007}) of  
using a Gaussian process-based response-surface approximation to the
model output. This approach results in that, conditional on $\n y^M$
and on a set of parameters specifying the Gaussian process, $y^M (\cdot)$ 
follows a Gaussian process with mean and covariance functions which
are available in closed form. The approach that \ourpack\ currently
implements for emulating $y^M(\cdot)$ estimates the unknown parameters,
denoted by $\n \theta^M$ and $\n \theta^L$, by maximum likelihood,
using the \proglang{R} package \pkg{DiceKrigging}
\citep{dice12}.
% , so that
% $$
% y^M (\n x, \n u)\mid \n y^M, \hat{\n \theta}^M, \hat{\n \theta}^L \sim
% N\big(\hat{m}(\n x,\n u), \hat{V}(\n x,\n u)\big) 
% $$
% where $\hat{m}$ is the (estimated) conditional expectation of $y^M (\n
% x, 
% \n u)$ given the model data, and similarly for the variance. 
% %  (derivation
% % is simple but tedious, details can be found in add references)

Analytic expressions for the mean and covariance functions can be
found in
Appendix~\ref{app.emu}, along with a description of the parameters
${\n\theta}^M$ and ${\n\theta}^L$. In practical terms, the output of
the computer model 
at a set of untested configurations (given $\n y^M$ and the parameter
estimates) follows a multivariate normal
distribution with known mean vector and covariance matrix. The
function \code{predictcode} returns iid draws from this multivariate
distribution, along with its mean vector and 
%the Cholesky decomposition of the
covariance matrix. 

% The estimation $\hat{\n\theta}^M$ is typically tackled
% by maximizing the likelihood function associated with the {\tt
%   model.data}.  
% In the literature the resulting fitted probabilistic
% model is called the emulator.

%The notion of calibrating a model stands for providing values of the
%calibration parameters which make the model produce closest results
%to reality. Hence, to address this problem it becomes necessary to
%introduce the idea of `reality'  

Field data consists of noisy observations of the real process,
possibly with replicates. To be more precise, we have a design set
$D^F=\{ {\n x}_{1}^{\star},\ldots,{\n x}_{n}^{\star}\}$ and we observe
$$
y^F(\n x_j^*) = y^R(\n x_j^*)+\varepsilon_{j},\quad j=1,\ldots,n
$$
where $y^R(\cdot)$ represents the real process and $\varepsilon_{j}$
are independent and identically distributed $N(0,1/\lambda^F)$ random
variables. % Both the field observations and the associated design are
% stored in a {\tt data.frame} that we refer to as {\tt field.data}. See
% Table~\ref{inv.dataframe} for an illustrative example of how
% the data is organized. 
Notice that the field data may contain
replicates, that is, independent  measurements of the experiment using
the same configuration of the controllable inputs. We denote the set of
field observations by $\n y^F$.

Calibrating a model stands for finding estimates of 
the vector of
calibration parameters based on field observations of the real
phenomenon. This is achieved by postulating a statistical model relating
the output of the model and the real phenomenon which introduces the
notion of model discrepancy (cf. \citet{Crai:1997},
\citet{Kenn:OHag:2001} and \citet{Gold:2010}), namely,
\begin{equation}\label{eqreal}
y^R(\n x)=y^M(\n x,\n u^\star)+b(\n x)\ ,
\end{equation}
where $b(\n x)$ stands for the bias or discrepancy function, and $\n
u^\star$ is the unknown value of the calibration vector which we are
ultimately interested in estimating. For ease of notation we refer to
$\n u^{\star}$ simply as $\n u$.

The approach that this package implements is (partially) Bayesian and
therefore 
requires the specification of a prior for all the unknown quantities
that appear in the statistical model, namely, $b(\cdot)$, $\n u$, and
$\lambda^F$. In line with \citet{Baya:2007}, we specify these priors
in a fashion that requires very little input from the user. An
exception is the prior on $\n u$ which should reflect expert opinion
about the calibration parameter. 
Details of the prior specification are
available in Appendix~\ref{app.prior}. Let $(\n u, \lambda^b,\n
\beta^b,\lambda^F)\equiv (\n u, \n \theta^F)$
denote the vector of 
unknown parameters at this stage of the analysis, which we refer to as
stage II. (The bias function $b(\cdot)$ gets a Gaussian process prior;
$\lambda^b$ denotes the precision and the vector $\n \beta^b$ controls
the correlation structure of this process.)

The posterior distribution is obtained using
Markov chain Monte Carlo (MCMC) methods and is ultimately represented by a 
sample of correlated draws, which we denote by $\{ \n u_i, \n \theta^F_i,\
i=1,\ldots,M\}$. Details on the sampling method used are described in
Appendix~\ref{app.mcmc}. The \ourpack\ function that implements this
task is called \code{bayesfit}.

Once the posterior distribution of all the unknowns in the model is
obtained, we can proceed to validate the computer model at $D^V$, a
set of configurations for the controllable inputs. To
do so, we must obtain draws from the distribution
\begin{equation}\label{real}
\int 
f(y^M(D_{\scriptsize \n u}^V),b(D^V)\mid \n y^M, \n y^F, \n u,\n \theta^F)\ \pi( \n
u,\n \theta^F\mid \n y^M, \n y^F)\ d\n
u\ d \n \theta^F
\end{equation}
which are obtained by drawing the vectors $\n y^M_i, \n b_i$ from
$f(y^M(D_{\scriptsize \n u_i}^V),b(D^V)\mid \n y^M, \n y^F, \n u_i,\n
\theta^F_i)$ 
for every $(\n u_i,\n \theta^F_i)$ in the previously constructed MCMC
sample drawn from the posterior distribution. The \ourpack\ function
that performs this task is called \code{predictreality}. Above, the set
$D^V_{{\scriptsize \n u}}$ corresponds to the design that results from
augmenting each of the configurations in $D^V$ 
with the vector $\n u$ 
for the calibration parameter. In general, we denote by $h(D)$ the
vector that results from evaluating the function $h$ at the elements
of $D$. 

Having obtained the sample $\{\n y^M_i, \n b_i,\ i=1,\ldots,M\}$, we can
compute several quantities which will aid us in the validation task 
\citep{Baya:2007}:
\begin{itemize}
\item The bias-corrected prediction of the real process (i.e., reality
 ) at $D^V$
$$
\hat{\n y}^R= \frac{1}{M} \sum_{i=1}^M(\n y^M_i + \n b_i)
$$
\item The tolerance bars measuring the accuracy of $\hat{\n y}^R$ as a
  predictor of $y^R(D^V)$ are computed as follows: pick $\gamma\in
  (0,1)$; then, compute $\n \tau = (\tau(\n x): \n x\in D^V)$ such that
  $(1-\gamma)\times 100\%$ of the samples satisfy
$$
|\hat{\n y}^R-(\n y^M_i + \n b_i)|\leq \n \tau\ ,
$$
with the inequality interpreted in a component-wise fashion.
We can then state that, for each $\n x \in D_V$, $\Pr(|y^R(\n
x)-\hat{y}^R(\n x)|<\tau(\n x)\mid \n y^F, \n y^M)=\gamma$.
\item The pure-model prediction of reality at $D^V$ is obtained by
  selecting an estimate of $\n u$, $\hat{\n u}$, say, which can be,
  for instance,
  its posterior mean or median. Then, output of the model at
  $D^V_{\hat{\n u}}$ is computed by either actually running the model
  or by exercising the emulator. Function \code{predictcode} obtains
  draws from the emulator, but also returns the mean vector of the
  emulator, which can be used as an estimate of the output of the
  model. Denote this estimate by $\hat{\n y}^M$.  
\item The tolerance bars measuring  the accuracy of $\hat{\n y}^M$ as a
  predictor of $y^R(D^V)$ are computed in a similar fashion: pick $\gamma\in
  (0,1)$; then, compute $\n \tau = (\tau(\n x): \n x\in D^V)$ such that
  $(1-\gamma)\times 100\%$ of the samples satisfy
$$
|\hat{\n y}^M-(\n y^M_i + \n b_i)|\leq \n \tau
$$
with the inequality interpreted in a component-wise fashion. We can
then state that, for each $\n x \in D_V$, $\Pr(|y^R(\n 
x)-\hat{y}^M(\n x)|<\tau(\n x)\mid \n y^F, \n y^M)=\gamma$.
 
\item It is also possible to estimate the bias associated with the
  pure-model prediction, $\n b_{\hat{\n u}}=y^R(D^V)-\hat{\n y}^M$:
    samples from its posterior predictive distribution can be obtained
    by computing $\{ \n y^M_i + \n b_i - \hat{\n y}^M\}$ so that a
    point estimate is $\hat{\n b}_{\hat{\n u}}=\hat{\n y}^R-\hat{\n y}^M$
      and $\gamma$ pointwise credible intervals can be determined by
      computing the associated 
      $\gamma/2\times 100\%$ and $(1-\gamma/2)\times 100\%$ sample
      quantiles.
\end{itemize}
The \ourpack\ function that, given $D^V$ and a posterior estimate of
${\n u}$, computes the 
bias-corrected prediction, the pure-model prediction, associated
tolerance bounds and estimated bias function is called \code{validate}.

\section{An overview of SAVE}\label{overview}

There are three high-level functions in \ourpack\ which allow
the user to perform all the tasks described in the previous section:
\code{SAVE}, \code{bayesfit} and \code{validate}. In general,
\begin{itemize}
\item \code{SAVE} creates an object of the class \code{SAVE-class}
  and essentially sets up the problem by filling out a number of slots
  of this object. The data structures are set up (more on this
  below). Maximum likelihood 
  calculations are performed using \pkg{DiceKriging}
  \citep{dice12}. These calculations serve two 
  purposes: fit the emulator of the
  computer model, and aid in the 
  specification of the prior of $\n \theta^F$
  (cf. Appendix~\ref{app.prior}). 
\item \code{bayesfit} produces a sample from the posterior
  distribution of the parameters $\lambda^F$, $\lambda^b$ and $\n
  u$ ($\n \beta^b$ is fixed at an estimate throughout the analysis,
  cf. Appendix~\ref{app.prior}). It takes as an argument an object of
  the class 
  \code{SAVE-class} 
  that has been 
  created using \code{SAVE}, and returns a copy of this object but with
  several additional slots filled out. These slots pertain to the MCMC
  sample obtained.
\item \code{validate} ultimately produces the bias-corrected
  prediction, the pure-model prediction, associated tolerance bounds
  and estimated bias function for any set of configurations for the
  controllable inputs and a posterior estimate of the vector of
  calibration inputs. It 
  performs this task based on the information contained in an object of the
  class \code{SAVE-class} which has been produced by a call of the function
  \code{bayesfit}.
\end{itemize}

Two additional functions are available in \ourpack, but these can be
considered low-level routines. The function \code{predictcode}
produces i.i.d.\ draws 
from the emulator evaluated at a set of design points, along with its
mean vector and covariance matrix. It expects
as an argument an object of the class \code{SAVE-class}. The function
\code{predictreality} expects as an argument an object of the class
\code{SAVE-class} which has been produced by the function
\code{bayesfit}, i.e., containing an MCMC sample. It outputs draws from
the distribution in \eqref{real} for a design set of configurations
for the controllable inputs of the problem. These functions are
internally called by \code{validate}, but can be utilized to further
explore the problem.

The output of each of these functions can be appropriately summarized
by customized calls to \code{print}, \code{summary}, \code{plot} and
\code{show}. %

The data is handled in the following way. \ourpack\ assumes that there
are two \proglang{R} data frames loaded: one containing all the field data,
and another containing all the model data. The response and the input
variables in the designs are identified by the \code{names} associated
with each of these data frames, so they must be consistent. For
illustrative purposes, consider
the synthetic example in Figure~\ref{inv.dataframe}. The
data frame at the top of the figure contains the field data and
the one at the bottom contains the model data. If we decide to analyze
the problem where the response is the variable is \code{expand};
controllable inputs are \code{temp}, \code{press} and \code{weight};
and calibration inputs are \code{delta1} and \code{shift}, then we
must call the function \code{SAVE} with arguments, \code{field.data =
  field}, \code{model.data = model}, \code{response.name = "expand"},
\code{controllable.names = c("temp", "press", "weight")}, 
\code{calibration.names = c("delta1", "shift")}.
Notice that not all the columns present in the data frames are
incorporated in this analysis.

\begin{figure}[t!]
\begin{center}
{\small\scalebox{0.75}{
\begin{tabular}{c}
\begin{tabular}{cccccc}
\multicolumn{6}{c}{data frame called \code{field} containing the field
  data}\\[.2cm]
& \code{temp} & \code{press} & \code{weight} & \code{expand} & \code{height}\\
1 & 35.1 & 2.65 & 600 & 84.1 & 5.1\\
2 & 35.1 & 2.75 & 600 & 90.6 & 9.2\\
3 & 35.1 & 2.65 & 600 & 80.4 & 6.1\\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
$n$ & 30.3 & 2.65 & 700 & 83.4 & 7.1\\
\end{tabular}\\[2cm]
\begin{tabular}{cccccccc}
\multicolumn{8}{c}{data frame called \code{model} containing the
    model data}\\[.2cm] 
& \code{temp} & \code{press} & \code{weight} & \code{delta1} & \code{shift}
          & \code{expand} & \code{delta2}\\ 
1 & 23.2 & 2.12 & 629.1 & 0.22 & -1.1 & 99.1 & 0.22\\
2 & 43.7 & 2.11 & 711.0 & 0.84 & -0.9 & 70.1 & 0.83\\
$\vdots$  & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ &
$\vdots$ & $\vdots$\\ 
$N$ & 34.4 & 2.12 & 700.1 & 0.49 & -0.8 & 67.6 & 0.33\\
\end{tabular}
\end{tabular}
}
}
\end{center}
\caption{{\small Synthetic example: the data frame at the top of the
    figure is called \code{field} and contains the field data; the data
    frame at the bottom of the 
    figure is called \code{model} and contains the model data}}  
\label{inv.dataframe}
\end{figure}


% There are three high-level functions in \ourpack, namely \code{SAVE},
% \code{bayesfit} and \code{validate}, two lower-level functions called
% \code{predictcode} and \code{predictreality}, plus a number of
% functions that assist the user in exploring the results obtained. 

% The function \code{SAVE} creates an object of the class
% \code{SAVE-class} and essentially sets up the model: one needs to
% specify data frames containing the model and the field data, which are
% the controllable and 
% the calibration inputs, a formula for the 
% mean function of the emulator, and a best guess for $\n u$ (cf.
% Appendix~\ref{app.prior} for details on this argument). The
% function 

\section{An example} \label{spotweld}
In this section we illustrate the use of the package with an analysis
of a real example. It is the so-called spotweld example 
originally analyzed in \citet{Baya:2007}. We refer the interested
reader to that paper for complete details on the application. 

In Figure~\ref{spot} you 
can find a schematic representation of the spot welding process. Two
sheets of metal of a particular thickness (\code{thickness}) are compressed by
two 
electrodes under a certain 
applied load (\code{load}). Electric current of magnitude \code{current} is
passed through 
said electrodes and the heat produced by the current flow causes the
surfaces under pressure to melt. After cooling, a weld nugget is
formed and as a result the two metal sheets are welded together. The
scientists are interested in the diameter of this nugget (\code{diameter}). 

Included in the package are two datasets, \code{spotweldfield} and
\code{spotweldmodel} that pertain, respectively, to field experiments
and computer model experiments associated with this problem. After
loading the package (\code{library(SAVE)}), the dataframes can be
loaded using the commands \code{data(spotweldfield,package="SAVE")}
and \code{data(spotweldmodel,package="SAVE")}.
Notice that the columns of the data frames are appropriately named,
and that the computer model features an additional input, named
\code{tuning}, which is a calibration input related to contact resistance. 

We start this analysis with setting up the problem by creating
\code{sw}--- an object of the class 
\code{SAVE-class}--- using the function \code{SAVE}:
\begin{CodeInput}
R> sw <- SAVE(response.name="diameter", controllable.names=c("current", "load", "thickness"),
+             calibration.names=c("tuning"), field.data=spotweldfield,
+             model.data=spotweldmodel, mean.formula=~1),
+             bestguess=list(t=4.0))
\end{CodeInput}

Here we are specifying which columns correspond to the response and
which correspond to the controllable and the calibration
inputs. Additionally, 
we are also 
\begin{itemize}
\item setting the mean function of the Gaussian process approximation to the
output of the computer model as a constant (with the option
\code{mean.formula=~1)}) and
\item providing an estimate of the vector of calibration inputs as a
  list (\code{bestguess}), which will be used in specifying the prior
  for $\n \theta^F$ 
  --- see Appendix~\ref{app.prior} for further details.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.2]{SW}
  \caption{Schematic representation of the spotwelding process\label{spot}}
\end{figure}

The object \code{sw} has now been created and several of its slots
have been filled. The easiest way of accessing that information is by
means of the command \code{summary(sw)}.

Since at this point the emulator has been fitted, we could now use the
function \code{predictcode} to predict 
the output of the computer model at a set of input configurations ---
more on this later. 
Instead, we will proceed to fit the Bayesian model \eqref{eqreal}
relating reality 
to computer model. To do so, we use the function \code{bayesfit}:
\begin{CodeInput}
R> swbayes <- bayesfit(object=sw, prior=c(uniform("tuning", upper=8,
+                      lower=0.8)), n.iter=20000, n.burnin=100, 
+                      n.thin=2)
\end{CodeInput}
We have created a new object --- \code{swbayes} --- but 
instead we could have updated the object we have just created,
\code{sw}. Notice that we need to specify a prior for the calibration
parameter \code{tuning}, and also options pertaining to the MCMC
algorithm. We have set a uniform prior for the calibration parameter,
20000 iterations for the MCMC with a burn-in period of 100 iterations,
and a thinning of 2. Other options were left at the corresponding
defaults --- cf Appendix~\ref{app.mcmc} for additional details.

The object \code{swbayes} now contains not only the estimates
previously computed using \code{SAVE} but also a sample from the
posterior distribution of the calibration parameter \code{tuning} and of the
field and bias precisions, $\lambda^F$ and $\lambda^b$,
respectively. To display this information we can again 
resort to the command \code{summary(swbayes)}. We can however also
plot the samples obtained: 
\begin{itemize}
\item \code{plot(swbayes, option="trace")} will
give us the traceplots;
\item \code{plot(swbayes, option="calibration")}
will produce an histogram of the posterior samples of the calibration
parameters and corresponding priors: see Figure~\ref{cal}; and,
\item  \code{plot(swbayes, option="precision")} will plot
histograms of the posterior samples of the field and bias
precisions. These histograms also include a plot of the prior and of
the estimates that are used in constructing the prior, see
Figure~\ref{prec}.
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{postcal.pdf}
  \caption{Posterior distribution of the calibration parameter
    \code{t}. The dotted line corresponds to the prior used.}
  \label{cal}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{postprec.pdf}
  \caption{Posterior distribution of the precisions. The dashed
    vertical lines indicate the estimates of the parameters; the
    dash-dot lines indicate the priors used.}
  \label{prec}
\end{figure}


Additionally, we can of course access the raw data. By running the
command \code{slotNames(swbayes)} we get a description of the
names of all the
slots of the object \code{swbayes}, and it's then clear how to obtain the
MCMC samples: \code{swbayes@mcmcsamples}.

After fitting the Bayesian model, we can finally produce predictions
of reality and also assess the quality of pure-model predictions of
reality. The package \pkg{SAVE} provides a very convenient function
that performs all these calculations. To illustrate its use in the
present 
example, please consider the following \proglang{R} code:
\begin{CodeInput}
R> load <- c(4.0,5.3); curr <- seq(from=20,to=30,length=20); g <- c(1,2)
R> xnew <- as.data.frame(expand.grid(curr,load,g))
R> names(xnew)<-c("current","load","thickness")
R> 
R> valsw <- validate(object=swbayes,newdesign=xnew,
+                    calibration.value="mean",n.burnin=100)
\end{CodeInput}
We first construct the design of controllable inputs at which we want
to predict reality. For four combinations of load and thickness of the
metal plates, we want to predict the weld diameter as a function of
current. Regarding the pure-model prediction, we are setting the
calibration parameters at the corresponding posterior mean. The
resulting object \code{valsw} contains a slot named \code{validate}
where a matrix is stored. This matrix contains as columns the
pure-model prediction of reality (\code{pure.model}) and associated
tolerance bound (\code{tau.pm}); the estimate of the bias associated
with the pure-model prediction and pointwise credible interval for
that unknown (\code{bias.Lower} and \code{bias.Upper}); the
bias-corrected prediction of  
reality (\code{bias.corrected}) and associated tolerance bounds
(\code{tau.bc}). This
information can be accessed using \code{summary(valsw)} but can also
be plotted (\code{plot(valsw)}) --- you can find the plot in
Figure~\ref{valplot}. Depending on the problem, this default plot will
not always be the most 
appropriate way 
of displaying the estimates. Nevertheless, since we have access to the
estimates, we can certainly construct customized plots for any problem at
hand. As an example, in Figure~\ref{valplotcustom} we can find a plot
of the pure-model prediction and associated tolerance bounds as a
function of current for the 4 different combinations of load and
thickness. The circles correspond to the appropriate field
observations.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{plotval.pdf}
  \caption{Default validation plot. The plot at the top contains the
    pure-model predictions (circles) at each of the input
    configurations in \code{xnew} and associated 90\% tolerance
    bounds. The middle plot depicts an estimate of the bias of these
    pure-model estimates. The bottom plot contains the bias-corrected
    prediction (circles) and associated 90\% tolerance bounds. }
  \label{valplot}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{valswcustom.pdf}
  \caption{Customized validation plot. Here we can see the pure-model
    for 4 different combinations of load and thickness as a function
    of current. The dotted lines indicate the 90\% tolerance bounds
    and the circles represent the observed field data correspoding to
    that particular combination of controllable inputs.}
  \label{valplotcustom}
\end{figure}


As stated above, \code{validate} is a function which calls two
low-level functions, \code{predictcode} and
\code{predictreality}. There may be situations where one must directly 
call these functions. We illustrate this in what follows.

Imagine that one is interested in understanding how the nugget
diameter \code{diameter} 
varies with the current \code{current}. One might assess this variation by
looking at the derivative of \code{diameter} with respect to \code{current}. Let's
do that for \code{thickness}=1 and \code{load}=4. We start by predicting reality
at an equally-spaced grid of \code{current} values between 20 and 30:
\begin{CodeInput}
R> aload <- 4; g <- 1; curr <- seq(from=20, to=30, length=80)
R> xnew <- expand.grid(curr,aload,g)
R> names(xnew) <- c("current","load","thickness")
R>   
R> prsw <- predictreality(object=swbayes, newdesign=xnew)
\end{CodeInput}

The object \code{prsw} has slots named \code{modelpred} and
\code{biaspred} where the draws from \eqref{real}, which we have
denoted
in Section~\ref{frame} by $\n y^M_i, \n b_i$, are stored. We now
obtain the 
corresponding derivatives with respect to current of each of these
draws, to finally obtain draws from the corresponding derivative of
reality:
\begin{CodeInput}
R> delta <- diff(curr)[1]
R> model <- prsw@modelpred
R> dmodel <- diff(t(model))/delta
R> bias <- prsw@biaspred
R> dbias <- diff(t(bias))/delta
R>
R> dreal <- dmodel + dbias
\end{CodeInput}
These draws can be summarized by computing the corresponding mean and
tolerance bounds as explained in Section~\ref{frame}. This is plotted
in Figure~\ref{deriv}. 

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{deriv.pdf}
  \caption{Bias-corrected prediction of the derivative of \code{N}
    with respect to \code{current} --- the estimate is the solid line; the
    dashed lines are the 90\% tolerance bounds. The pure-model
    prediction is the dash-dotted line.}
  \label{deriv}
\end{figure}

Additionally, we can use \code{predictcode} to obtain draws from the
emulator evaluated at a posterior estimate of the calibration input,
\code{tuning}. This allows us to produce the pure-model estimate of the
derivative:
\begin{CodeInput}
R> u <- 3.2; aload <- 4; g <- 1; 
R> xnewpure <- expand.grid(curr,aload,g,u)
R> names(xnewpure) <- c("current","load","thickness","tuning")
R>
R> pmsw <- predictcode(object=swbayes,newdesign=xnewpure,n.iter=20000)
R>
R> puremodel <- pmsw@samples
R> dpuremodel <- diff(t(puremodel))/delta
\end{CodeInput}
The mean of the samples in \code{dpuremodel} is the pure-model prediction
of the derivative. This is plotted in Figure~\ref{deriv}. Notice how
the two estimates, pure-model and bias-corrected, are in this case
even qualitatively different.
\section*{Acknowledgements}
This research was funded by Portuguese National Funds through FCT - Funda\c
c\~ao para a Ci\^encia e a Tecnologia, project PTDC/MAT/105349/2008
and by the Spanish Ministry of Education and Science under grant
MTM2010-19528. An initial version of this software was produced while
the authors were postdoctoral researchers at the National Institute of
Statistical Sciences (NISS), and at the times their research was
supported in part by 
grants from General Motors and the National Science Foundation
(Grant DMS-0073952). M.J. Bayarri (Valencia), James O.\ Berger (Duke),
Jerry Sacks (NISS) were 
actively involved in that research, and so were J.A. Cafeo,
J.\ Cavendish, C.H.\ Lin and J.\ Tu from General Motors. 
\appendix
\section{Details on the emulator} \label{app.emu} We assume that 
\textit{a priori} 
$y^M(\cdot)$ follows a stationary Gaussian process with mean and
covariance functions governed by unknown parameters
$\n{\theta}^L$ and $\n{\theta}^M=(\lambda^M, \n{\alpha}^M,
\n{\beta}^M)$, respectively. The mean function of the Gaussian process
is assumed to be of the 
form $\n{\Psi}'(\n{\cdot})\n{\theta}^L$ where
$\n{\Psi}(\n{z})$ is a specified  $k \times 1$ vector
function of the input $\n{z}=(\n x, \n u)$ and $\n{\theta}^L$ is a $k
\times 1$ vector of unknown regression parameters. This mean function
is specified through the argument \code{mean.formula} of the
\code{SAVE} function. Note that one of the restrictions of \ourpack\
is that $\n \Psi$ can only be function of $\n x$, the vector of
controllable inputs. 

The parameter $\lambda^M$ is the precision (the inverse of the
variance) of the Gaussian process and the other parameters
$(\n{\alpha}^M, \n{\beta}^M)$ control the correlation function of
the Gaussian process, which we assume to be of the form
\begin{equation*}
c^M(\n{z},\n{z^{\star}})
=\exp\left(-\sum_{j=1}^d\beta^M_j |
z_j-{z_j}^{\star}|^{\alpha_j^M}\right).
\end{equation*}
Here, $d$ is the number of coordinates in $\n{z}=(\n x,\n u)$,
the $\alpha^M_j$ are numbers between 0 and 2, and the $\beta^M_j$ are
positive parameters.

After observing $\n y^M$, the conditional posterior distribution
of $y^M$ given the hyperparameters, $f(y^M(\cdot) \mid \n{y}^M,
\n{\theta}^L,\n{\theta}^M)$, is a Gaussian process with
updated mean and covariance functions given respectively
\begin{align*}
& \E[y^M(\n{z}) \mid \n{y}^M,
\n{\theta}^L, \n{\theta}^M] =\n{\Psi}'(\n{z}) \,
\n{\theta}^L +
{\n{r}_z}'(\n{\Gamma}^M)^{-1}(\n{y}^M-\n{X}\n{\theta}^L)\\
&\COV[\, y^M(\n{z}), y^M(\n{z}^{\star})
\mid \n{y}^M, \n{\theta}^L, \n{\theta}^M] =
\frac{1}{\lambda^M} \, c^M(\n{z},\n{z}^{\star}) -
{\n{r}_z}'(\n{\Gamma}^M)^{-1}\n{r}_{z^{\star}},
\end{align*}
where ${\n{r}_z}' = \frac{1}{\lambda^M} \ (c^M(\n{z},
\n{z}_1),\ldots,c^M(\n{z},\n{z}_N))$, $\n{\Gamma}^M$ is
given above and $\n{X}$ is the matrix
with rows $\n{\Psi}'(\n{z}_1),\ldots, \n{\Psi}'(\n{z}_N)$. 

To obtain an emulator for
$y^M$, we replace in the formulae above the unknown parameter values by the
corresponding maximum likelihood estimates.

\section{Details on the stage II prior}\label{app.prior}
The stage II unknowns are $b(\cdot)$, the bias function, $\n u$, the
vector 
of calibration parameters, and $\lambda^F$, the precision of the field
measurement error. The prior for $\n u$ is specified using expert
knowledge. Currently, the distributional choices are limited to
uniform and normal, this last one truncated to an interval. This
enters the function \code{bayesfit} through argument \code{prior}.

The prior for the bias is a stationary zero-mean
Gaussian process with covariance $\lambda^b$ and correlation function
given by
$$
c^b(\n{x},\n{x^{\star}})
=\exp\left(-\sum_{j=1}^p\beta^b_j |
x_j-{x_j}^{\star}|^{2}\right).
$$
Here, $p$ is the number of coordinates in $\n{x}$, and the $\beta^b_j$
are positive parameters. Let $\n
\beta^b=(\beta_1^b,\ldots,\beta_p^b)$. We need to specify a prior for
$\n\theta^F=(\lambda^b$, $\n \beta^b$, $\lambda^F)$, and we do so in a
nearly automatic 
fashion as follows: we start by selecting a best guess for the vector
of calibration parameters, denoted by $\tilde{\n u}$, which is the
argument 
\code{bestguess} is function \code{SAVE}. Then, using the
emulator, we predict the output of the computer model at
$D^F_{\scriptsize \tilde{\n u}}$, denoted $y^M(D^F_{\scriptsize
  \tilde{\n u}})$. Next, treat $\n y^F-y^M(D^F_{\scriptsize
  \tilde{\n u}})$ as a realization of a Gaussian process with a
nugget, namely as a realization of a multivariate normal with mean
zero and covariance matrix $c^b(D^F)/\lambda^b+I/\lambda^F$ to get
maximum likelihood estimates $\hat{\lambda}^b$, $\hat{\n \beta}^b$,
$\hat{\lambda}^F$. Then,
\begin{itemize}
\item $\n \beta^b$ is fixed throughout the analysis at $\hat{\n
    \beta}^b$;
\item $\lambda^b$ and $\lambda^F$ are independent exponentially
  distributed quantities centered at a multiple of the corresponding
  estimates,  
  $\hat{\lambda}^b$ and $\hat{\lambda}^F$. This multiple is set
  through the parameter \code{mcmcMultmle} in \code{bayesfit} and its
  purpose is to allow the user to specify a prior which is relatively
  flat in the region 
  where the posterior distribution accumulates.
\end{itemize}

The function \code{SAVE} computes these maximum likelihood
estimates (with the help of the package \pkg{DiceKriging},
\citet{dice12}) and stores 
these in the 
slot \code{mle} of the corresponding 
object of class \code{SAVE-class}. 
% The best guess of $\n u$,
%$\tilde{\n u}$, is the argument 
%\code{bestguess} of this function.

\section{Details on the MCMC}\label{app.mcmc}
Full details on the sampling mechanism can be found in
\citet{Baya:2007}. The algorithm implemented in \ourpack\ requires
very little input apart from the necessary length of the
simulation, burn-in and thinning numbers. This is because all unknowns
are sampled directly from their 
full conditionals with the exception of the vector of calibration
parameters. This vector is sampled using a Metropolis-Hastings step, for which
the user needs to decide on three aspects:
\begin{itemize}
\item The proposal distribution is a mixture between the prior and a
  local 
  move. The user needs to specify the probability of sampling from the
  prior, which is argument \code{prob.prop} of the \code{bayesfit}
  function;
\item The algorithm performs a fixed number of Metropolis-Hastings
  steps before deciding on a move; the user must set this number,
   and this is argument \code{nMH} of the \code{bayesfit}
  function;
\item The package implements two alternative methods: \code{method=2}
  specifies that computer model and bias are analytically integrated
  out before sampling $\n u$; this is the default and preferred
  method. If \code{method=1}, these vectors are not integrated out.
\end{itemize}

\bibliography{Computermodels,biblio} 
 

\end{document}


Before we can run these computer models, we have to
specify a vector of inputs which typically includes calibration
parameters. Scientists are thus often interested in combining data
obtained in 
computer model runs and in physical experiments to determine estimates of these
calibration parameters which make the output of the model ``match'' in
some sense the real process. This is what is meant by calibration of a
computer model, a difficult statistical question dealt by \pkg{SAVE}. 

\pkg{SAVE} implements the methodology for calibration developed in 
\citet{Crai:1996}, \citet{Kenn:OHag:2001},
\citet{Kenn:OHag:Higg:2002}, \citet{Higd:2004}, and most directly in
\citet{Baya:2007}. It hinges on modeling the relationship between reality and
computer model output in a Bayesian fashion introducing the notion of
bias, effectively combining model and field data to produce estimates
of the calibration parameters. The package allows the user to
perform posterior inference, based on Markov chain Monte Carlo (MCMC)
techniques, on the unknown
parameters of the statistical model, including calibration inputs, and to draw
samples from the posterior predictive distribution of the bias
function and of an approximation to the computer
model.
